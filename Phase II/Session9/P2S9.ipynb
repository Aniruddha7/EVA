{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2S9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtGu6y7EUeXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import variable\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4wSx6oIWhTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "  def __init__(self, max_size = 1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self,transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, \\\n",
        "        batch_dones = [], [], [], []\n",
        "    for i in ind:\n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy = False))\n",
        "      batch_next_states.append(np.array(next_state, copy = False))\n",
        "      batch_actions.append(np.array(action, copy = False))\n",
        "      batch_rewards.append(np.array(reward, copy = False))\n",
        "      batch_dones.append(np.array(done, copy = False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), \\\n",
        "        np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), \\\n",
        "            np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8V3YcD4b-nV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  def __init__(self, state_dims, action_dim, max_action):\n",
        "    #max_action is to clip in case we added too much noise\n",
        "    super(Actor, self).__init__() #activate the inheritance\n",
        "    self.layer_1 == nn.Linear(state_dims, 400)\n",
        "    self.layer_2 == nn.Linear(400, 300)\n",
        "    self.layer_3 == nn.Linear(300, action_dim)\n",
        "    self.max_action == max_action\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action*torch.tanh(self.layer_3(x))\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7lzSnpWvxc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "  def __init__(self, state_dims, action_dim):\n",
        "    #max_action is to clip in case we added too much noise\n",
        "    super(Critic, self).__init__()#activate the inheritance\n",
        "    #First critic network\n",
        "    self.layer_1 = nn.Linear(state_dims + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    #Second critic network\n",
        "    self.layer_4 = nn.Linear(state_dims + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, action_dim)\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  def forward(self, x, u):#x - state, u= action\n",
        "    xu = torch.cat([x,u], 1)#1 for vertical concatination, 0 for Horizontal\n",
        "    #Forward propagation on first critic\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    #Forward propagation in second critic\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):#x - state, u= action This is used for updating Q vaules \n",
        "    xu = torch.cat([x, u], 1)#1 for vertical concatination, 0 for Horizontal\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxG15uH9ygC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Selection the device(CPU or GPU)\n",
        "device = torch.device('cuda'if torch.cuda.is.available() else 'cpu')\n",
        "\n",
        "#Building the whole training process into a class\n",
        "class T3D(object):\n",
        "  def __init__(self, state_dims, action_dim, max_action):\n",
        "    #making sure our T3D lcass can work with any env\n",
        "    self.actor = Actor(state_dims, action_dim, max_action).to(device) #GD\n",
        "    self.actor_target = Actor(state_dims, action_dim, max_action).to(device) #Polyak avg\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict)\n",
        "    #Initializing with model weights to keep them same\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "\n",
        "    self.critic = Critic(state_dims, action_dim).to(device) #GD\n",
        "    self.critic_target = Critic(state_dims, action_dim).to(device) #Polyak avg\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict)\n",
        "    #Initializing with model weights to keep them same\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action == max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state =  torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).CPU().data.numpy().flatten()\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-46eJUDy2NXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau = 0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "  for it in range(iterations):\n",
        "    #Step 4 we smaple from a batch of transitions (s, s', a, r) from memory\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones \\\n",
        "         = replay_buffer.sample(batch_size)\n",
        "    state = torch.Tensor(batch_states).to(device)\n",
        "    next_state = torch.Tensor(batch_next_states).to(device)\n",
        "    action = torch.Tensor(batch_actions).to(device)\n",
        "    reward = torch.Tensor(batch_rewards).to(device)\n",
        "    done = torch.Tensor(batch_dones).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7blG3aE34o5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step 5: From next state S', the actor terget plays the next actions a'\n",
        "next_action = self.actor_target.forward(next_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud60p3z15B1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step 6: We can add Gaussian noise to this next action a' and we clamp it in a range of values supported by the environment\n",
        "noise = torch.Tensor(batch_actions).data.normal_(0,policy_noise).to(device)\n",
        "noise = noise.clamp(-noise_clip, noise_clip)\n",
        "next_action = (next_action + noise).clamp(-self.max_action, self.max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rPzN4OB6rvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step 7: The two critic targetstake each other (s', a') as input and return two Q values, Qt1(s', a') Qt2(s', a')\\\n",
        "target_Q1, target_Q2 = self.critic.target.forward(next_state, next_action)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDF2FwjJ7S1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step 8: We keep the minimum of these two Q values\n",
        "target_Q = torch.min(target_Q1, target_Q2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wap2CRPB7jMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step 9: We get the final target of two critic model, which is :\n",
        "#Qt =r+gamma*min(Qt1, Qt2)\n",
        "#target_Q = reward + (1-done) * discount*target_Q\n",
        "#0 = episode not over, 1 = episode over\n",
        "# We can't run the above equation efficiently as some components are in computational graphs and some are not. We need to make one minor modification\n",
        "target_Q = reward +((1-done)* discount* target_Q).detach()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiqjt_e680xT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step 10: Two critic model take each the couple(s, a), as input and return two Q values\n",
        "current_Q1, current_Q2 = self.critic.forward(state, action)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BblAudWQ9Qed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step 11: We compute the loss coming from the twso critic models\n",
        "critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NMpYyGa-TsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step12: We backpropagate this critic loss and update the parameters of the critic model with Adam optimizer\n",
        "self.critic_optimizer.zero_grad() #initializing the gradients to zero\n",
        "critic_loss.backward()#Computing the gradients\n",
        "self.critic_optimizer.step()#Performing the weight updates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8VNBTs5_C4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step13: Once every two iterations we update our Actor model by performing gradient ascent on the output of the first critic model\n",
        "if it % policy_freq ==0:\n",
        "  #This is DPG part\n",
        "  actor_loss = -(self.critic.Q1(state,, self.actor(state)).mean())\n",
        "  self.actor_optimizer.grad_zer()\n",
        "  actor_loss.backward()\n",
        "  self.actor_optimizer.step()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJjQgJghAnw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step14: Still once every two iterations we update our Actor model by performing Polyak averaging\n",
        "for param, target_param in zip(self actor.parameters(), self actor_target.parameters()):\n",
        "  target_param.data.copy_(tau * param.data + (1 - tau) * taget_param.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjOkU1rxAoAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step15: Still once every two iterations we update our Critic model by performing Polyak averaging\n",
        "for param, target_param in zip(self critic.parameters(), self critic_target.parameters()):\n",
        "  target_param.data.copy_(tau * param.data + (1 - tau) * taget_param.data)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
